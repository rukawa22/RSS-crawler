import feedparser
import gspread
from google.oauth2.service_account import Credentials
import trafilatura
from datetime import datetime, timedelta
import time
import os
import json

# --- éš±ç§è¨­å®šå€åŸŸ (å„ªå…ˆè®€å– GitHub Secrets) ---
SHEET_ID = os.getenv('SHEET_ID')
SHEET_NAME = 'RSS'

RSS_URLS = [
    'https://news.cnyes.com/rss/category/tw_stock', 
    'https://www.ctee.com.tw/rss/news', 
    'https://tw.stock.yahoo.com/rss?category=tw-market', 
    'https://technews.tw/category/component/feed/', 
    'https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664', 
    'https://finance.yahoo.com/news/rssindex', 
    'https://news.google.com/rss/search?q=China+economy&hl=zh-TW&gl=TW&ceid=TW:zh-Hant' 
]

def get_google_sheet():
    scopes = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    
    # çµ±ä¸€é‡‘é‘°è®€å–é‚è¼¯ (è§£æ±º GitHub Actions å ±éŒ¯)
    creds_json = os.getenv('GOOGLE_CREDS_JSON')
    if creds_json:
        creds_dict = json.loads(creds_json)
        creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
    else:
        # æœ¬åœ°é–‹ç™¼ç”¨
        creds = Credentials.from_service_account_file('creds.json', scopes=scopes)
    
    client = gspread.authorize(creds)
    sh = client.open_by_key(SHEET_ID)
    
    try:
        worksheet = sh.worksheet(SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        worksheet = sh.add_worksheet(title=SHEET_NAME, rows="5000", cols="4")
        worksheet.append_row(["æ—¥æœŸ", "æ¨™é¡Œ", "å…§æ–‡æ‘˜è¦", "é€£çµ"])
        
    titles = worksheet.col_values(2) # ç²å–ç¾æœ‰æ¨™é¡Œä»¥å»é‡
    return worksheet, titles

def fetch_full_text(url):
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            content = trafilatura.extract(downloaded)
            return content if content else "ç„¡æ³•è§£æå…§æ–‡"
    except Exception as e:
        print(f"è§£æå¤±æ•— {url}: {e}")
    return "æŠ“å–å…§æ–‡å‡ºéŒ¯"

def cleanup_old_data(worksheet):
    try:
        total_rows = len(worksheet.get_all_values())
        if total_rows > 5000:
            print(f"æ¸…ç†èˆŠè³‡æ–™ä¸­... ç›®å‰è¡Œæ•¸: {total_rows}")
            worksheet.delete_rows(5001, total_rows)
    except Exception as e:
        print(f"æ¸…ç†å¤±æ•—: {e}")

def main():
    try:
        worksheet, existing_titles = get_google_sheet()
        print(f"æˆåŠŸé€£ç·šï¼ç›®å‰å·²æœ‰ {len(existing_titles)} å‰‡èˆŠç´€éŒ„ã€‚")
    except Exception as e:
        print(f"é€£ç·šå¤±æ•—: {e}")
        return

    new_data = []
    # å¼·åˆ¶ç²å–å°ç£æ™‚é–“ (UTC+8)
    tw_time = datetime.utcnow() + timedelta(hours=8)
    dt_str = tw_time.strftime('%Y-%m-%d %H:%M:%S')

    for rss_url in RSS_URLS:
        print(f"\n[æƒæ] {rss_url}")
        feed = feedparser.parse(rss_url)
        
        # ç¶­æŒåŸå§‹çš„å€’åºè®€å–é‚è¼¯
        for entry in reversed(feed.entries):
            title = getattr(entry, 'title', 'ç„¡æ¨™é¡Œ').strip()
            link = getattr(entry, 'link', '')
            
            if not link or title in existing_titles:
                continue 

            print(f"  - æ–°æ–‡ç« : {title[:20]}...")
            full_content = fetch_full_text(link)
            # å°‡æœ€æ–°æŠ“åˆ°çš„è³‡æ–™æ”¾å…¥ list
            new_data.append([dt_str, title, full_content, link])
            time.sleep(0.5) 

    if new_data:
        # é€™è£¡åè½‰ new_data ç¢ºä¿é€™ä¸€æ‰¹è£¡ã€Œæœ€æ–°ã€çš„åœ¨æœ€å‰é¢
        new_data.reverse()
        print(f"\næ­£åœ¨æ’å…¥ {len(new_data)} ç­†æ–°è³‡æ–™åˆ°é ‚ç«¯...")
        # æ’å…¥åœ¨ç¬¬äºŒè¡Œ (æ¨™é¡Œåˆ—ä¸‹æ–¹)ï¼Œé€™æœƒç¢ºä¿æœ€æ–°çš„è³‡æ–™æ°¸é åœ¨æœ€ä¸Šé¢
        worksheet.insert_rows(new_data, row=2)
        cleanup_old_data(worksheet)
        print("ğŸ‰ æ›´æ–°å®Œæˆï¼")
    else:
        print("\nç›®å‰æ²’æœ‰æ–°çš„æ–‡ç« ã€‚")

if __name__ == "__main__":
    main()
